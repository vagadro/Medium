{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b59ae2",
   "metadata": {},
   "source": [
    "# Word2Vec Model- Self Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e731f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "corpus = [\n",
    "     'this coffee is very bad and is expensive',\n",
    "     'this coffee is not bad and is cheap',\n",
    "     'this coffee is not amazing and is not affordable',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73714d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'coffee', 'is', 'very', 'bad', 'and', 'is', 'expensive'], ['this', 'coffee', 'is', 'not', 'bad', 'and', 'is', 'cheap'], ['this', 'coffee', 'is', 'not', 'amazing', 'and', 'is', 'not', 'affordable']]\n"
     ]
    }
   ],
   "source": [
    "# split each document/review into a list containing all the words in it\n",
    "i=0\n",
    "list_of_sentance=[]\n",
    "for sentance in corpus:\n",
    "    list_of_sentance.append(sentance.split())\n",
    "print(list_of_sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81f8acc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 0.19918270409107208), ('coffee', 0.0749121904373169), ('bad', 0.060690056532621384), ('expensive', 0.044891368597745895), ('not', 0.03371307626366615), ('is', 0.027107110247015953), ('very', 0.02671417035162449), ('affordable', 0.008842861279845238), ('this', -0.0684460774064064), ('amazing', -0.1445155292749405)]\n",
      "['is', 'not', 'and', 'coffee', 'this', 'bad', 'affordable', 'amazing', 'cheap', 'expensive', 'very']\n"
     ]
    }
   ],
   "source": [
    "# train the Word2Vec model \n",
    "\n",
    "w2v_model=Word2Vec(list_of_sentance,min_count=1)\n",
    "print(w2v_model.wv.most_similar('cheap'))   # check if similarity of any word is being outputed\n",
    "w2v_words= list(w2v_model.wv.index_to_key)  \n",
    "print(w2v_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5518a62e",
   "metadata": {},
   "source": [
    "# Average Word2Vector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c7023fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3002.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in tqdm(list_of_sentance): # for each review/sentence\n",
    "    sent_vec = np.zeros(100) # as word vectors are of zero length 100, you might need to change this to 300 if you use google's w2v\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors))          # to check the total rows of the matrix= this should be 3 as there are 3 docs in corpus\n",
    "print(len(sent_vectors[0]))       # this should be 100 as for each word, the respective vector is of 100 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7233e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4768da2e",
   "metadata": {},
   "source": [
    "# TFIDF weighed W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9abb01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tfidf vectorizer to get the tf-idf value of each word\n",
    "model = TfidfVectorizer()\n",
    "model.fit(corpus)\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "dictionary = dict(zip(model.get_feature_names(), list(model.idf_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "779c3302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2997.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = model.get_feature_names() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "for sent in tqdm(list_of_sentance): # for each review/sentence \n",
    "    sent_vec = np.zeros(100) # as word vectors are of zero length 100\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = w2v_model.wv[word]\n",
    "#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n",
    "            # to reduce the computation we are \n",
    "            # dictionary[word] = idf value of word in whole courpus\n",
    "            # sent.count(word) = tf valeus of word in this review\n",
    "            tf_idf = dictionary[word]*(sent.count(word)/len(sent))\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30caa148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(tfidf_sent_vectors))   # check the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f94c54dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.17668086e-03  2.87485251e-04  4.60832264e-04  1.41068221e-03\n",
      " -4.30968042e-03 -3.77369269e-03  1.54168978e-03  5.95426867e-03\n",
      " -3.95025615e-03 -3.29107748e-03  3.29879719e-03 -1.19683723e-03\n",
      " -9.02379144e-04  3.46718890e-04 -8.91748690e-04 -6.62799837e-04\n",
      "  2.16563512e-03 -1.26463881e-03 -2.69984581e-03 -5.88545786e-03\n",
      "  8.32185579e-04  5.86956014e-04  4.68625108e-03  1.17613927e-03\n",
      "  1.20826747e-03 -1.68935600e-03  1.55392750e-04  2.99458320e-03\n",
      " -2.85441464e-03 -3.26503726e-04 -1.46479376e-04 -1.32396257e-03\n",
      "  3.57426716e-03 -2.12316555e-03 -7.74603864e-04  1.88076661e-03\n",
      "  4.57043870e-03 -1.06206014e-03 -8.51976531e-04 -5.00718657e-04\n",
      " -1.84547632e-03  1.71285769e-03 -5.54864535e-03 -2.24105491e-04\n",
      "  3.83053012e-04 -1.10052999e-03 -2.90348600e-03  4.36247947e-03\n",
      "  8.90942183e-04  3.31094123e-03 -2.05762030e-03  1.76389095e-03\n",
      " -2.39405994e-03  8.00728418e-04  1.44202046e-03 -3.57695446e-03\n",
      "  4.61037317e-04 -4.12075947e-03 -4.09142425e-04  1.53882782e-03\n",
      " -5.06672412e-04  2.53423285e-04  2.87772049e-03 -3.35982758e-03\n",
      " -2.76625731e-03  1.34364299e-03 -7.41439508e-04  1.63612350e-03\n",
      " -2.23090706e-03 -9.25029319e-04  3.36754336e-03  6.25859272e-03\n",
      " -7.31208057e-04 -1.24807046e-03  1.67794361e-03  2.46726239e-03\n",
      "  2.87295451e-03 -1.04048882e-04 -4.11720552e-04 -2.17036303e-03\n",
      "  8.18422087e-04  2.36717283e-04  3.14244769e-03  3.60686447e-03\n",
      " -3.87868299e-03 -1.97095602e-03  7.34987993e-04  5.69251131e-05\n",
      "  2.74397032e-04  2.88261168e-03  5.92538112e-04  2.50864673e-03\n",
      "  1.85957669e-03  7.29566208e-04  6.72135744e-03  9.76477609e-04\n",
      " -1.63215827e-03 -3.50035247e-03  3.10831459e-03 -3.22878158e-05]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_sent_vectors[0])    # sample check tf-idf weighted word2vec vector for review 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c1a28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
